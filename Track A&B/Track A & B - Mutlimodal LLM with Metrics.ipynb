{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7FWCpD1bCuMoo6m51UxSj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 8 — Multimodal LLM with Metrics\n",
        "\n",
        "**Tracks Completed:** A (Speech)  and  B (Visualization)\n"
      ],
      "metadata": {
        "id": "32C_P3th6iHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Ups"
      ],
      "metadata": {
        "id": "KskVegkA6ug_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q SpeechRecognition gTTS transformers torch pandas matplotlib sentence-transformers jiwer\n",
        "\n",
        "import os, time, json, shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import speech_recognition as sr\n",
        "\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "import torch, matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from jiwer import wer\n",
        "\n"
      ],
      "metadata": {
        "id": "MO6rCWXu6qav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Loading & Helper Functions"
      ],
      "metadata": {
        "id": "et_1BsGV7RLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on:\", device)\n",
        "\n",
        "# Load TinyLlama\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    low_cpu_mem_usage=True\n",
        ").to(device)\n",
        "\n",
        "\n",
        "def run_llm(prompt):\n",
        "    \"\"\"\n",
        "    Generates an answer using TinyLlama in chat format.\n",
        "    \"\"\"\n",
        "    template = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
        "    inputs = tokenizer.encode(template, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Remove the input part so only the model's answer remains\n",
        "    if \"<|assistant|>\" in response:\n",
        "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "# --- Speech helper functions ---\n",
        "def speech_to_text(audio_path):\n",
        "    r = sr.Recognizer()\n",
        "    with sr.AudioFile(audio_path) as s:\n",
        "        audio = r.record(s)\n",
        "    return r.recognize_google(audio)\n",
        "\n",
        "\n",
        "def text_to_speech(text, out_path=\"audio_outputs/reply.mp3\"):\n",
        "    os.makedirs(\"audio_outputs\", exist_ok=True)\n",
        "    gTTS(text).save(out_path)\n",
        "    return out_path\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S6jlEhv77YsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Evaluation Cell"
      ],
      "metadata": {
        "id": "afH9PoH17cQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os, time, pandas as pd, numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# --- Load data ---\n",
        "df = pd.read_csv(\"text_inputs.csv\")  # must have 'question' and 'gold_answes'\n",
        "semantic_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "results = []\n",
        "\n",
        "# --- Loop through each question ---\n",
        "for i, row in df.iterrows():\n",
        "    q = row[\"question\"]\n",
        "    gold = row[\"gold_answers\"]\n",
        "\n",
        "    print(f\"\\nQ{i+1}: {q}\")\n",
        "    start = time.time()\n",
        "    reply = run_llm(q)\n",
        "    latency = round(time.time() - start, 2)\n",
        "\n",
        "    # Compute semantic accuracy per question\n",
        "    sim = util.cos_sim(\n",
        "        semantic_model.encode(gold, convert_to_tensor=True),\n",
        "        semantic_model.encode(reply, convert_to_tensor=True)\n",
        "    ).item()\n",
        "    semantic_accuracy = round(sim, 3)\n",
        "\n",
        "    # Add ablation-style columns (baseline placeholders)\n",
        "    record = {\n",
        "        \"Variant\": \"Baseline_TextOnly\",\n",
        "        \"Question\": q,\n",
        "        \"Gold Answer\": gold,\n",
        "        \"Reply\": reply,\n",
        "        \"Latency (s)\": latency,\n",
        "        \"Accuracy\": semantic_accuracy,\n",
        "        \"Visualization Quality\": \"N/A\",\n",
        "        \"Speech Accuracy\": \"N/A\",\n",
        "        \"Notes\": \"Text-only TinyLlama reasoning\"\n",
        "    }\n",
        "\n",
        "    results.append(record)\n",
        "    print(f\"A: {reply}\")\n",
        "\n",
        "# --- Save final output ---\n",
        "os.makedirs('text_outputs', exist_ok=True)\n",
        "output_path = \"text_outputs/baseline_full_metrics.csv\"\n",
        "pd.DataFrame(results).to_csv(output_path, index=False)\n",
        "\n",
        "\n",
        "# --- Summary view ---\n",
        "df_text = pd.DataFrame(results)\n",
        "avg_latency = round(df_text[\"Latency (s)\"].mean(), 2)\n",
        "avg_sem_acc = round(df_text[\"Accuracy\"].mean(), 2)\n",
        "print(f\"\\nAverage Latency: {avg_latency}s\")\n",
        "print(f\"Average Accuracy: {avg_sem_acc}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lXvF7gcX7VbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Speech Evaluation Cell (Track A)"
      ],
      "metadata": {
        "id": "UJV5vTs0-qfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Load reference text and answers (baseline CSV) ---\n",
        "ref_df = pd.read_csv(\"text_inputs.csv\")  # must contain 'question' and 'gold_answer'\n",
        "questions = ref_df[\"question\"].tolist()\n",
        "gold_answers = ref_df[\"gold_answers\"].tolist()\n",
        "\n",
        "# --- Load semantic model (rename to avoid conflict with LLM model) ---\n",
        "semantic_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "speech_results = []\n",
        "\n",
        "# --- Iterate through your audio folder ---\n",
        "for idx, audio_file in enumerate(sorted(os.listdir(\"audio_inputs\"))):\n",
        "    if not audio_file.endswith(\".wav\"):\n",
        "        continue\n",
        "\n",
        "    path = f\"audio_inputs/{audio_file}\"\n",
        "    print(f\"\\nProcessing: {audio_file}\")\n",
        "\n",
        "    # Speech → Text (transcription)\n",
        "    start = time.time()\n",
        "    try:\n",
        "        transcription = speech_to_text(path)\n",
        "    except Exception as e:\n",
        "        print(\"Speech recognition failed:\", e)\n",
        "        continue\n",
        "\n",
        "    # LLM reasoning\n",
        "    reply = run_llm(transcription)\n",
        "    latency = round(time.time() - start, 2)\n",
        "\n",
        "    # Text → Speech (save spoken reply)\n",
        "    output_file = f\"audio_outputs/reply_{os.path.splitext(audio_file)[0]}.mp3\"\n",
        "    text_to_speech(reply, out_path=output_file)\n",
        "\n",
        "    print(\"Reply:\", reply)\n",
        "    display(Audio(output_file, autoplay=False))\n",
        "\n",
        "    #  Compute Speech Accuracy (1 − WER between expected question and transcript)\n",
        "    #  If order of audios matches text_inputs.csv questions\n",
        "    ref_text = questions[idx]\n",
        "    speech_accuracy = round(1 - wer(ref_text.lower(), transcription.lower()), 3)\n",
        "\n",
        "    # Compute Semantic Accuracy (LLM output vs gold answer)\n",
        "    gold = gold_answers[idx]\n",
        "    sim = util.cos_sim(\n",
        "        semantic_model.encode(gold, convert_to_tensor=True),\n",
        "        semantic_model.encode(reply, convert_to_tensor=True)\n",
        "    ).item()\n",
        "    semantic_acc = round(sim, 3)\n",
        "\n",
        "    # Store metrics\n",
        "    record = {\n",
        "        \"Variant\": \"With_Speech\",\n",
        "        \"Question\": ref_text,\n",
        "        \"Gold Answer\": gold,\n",
        "        \"Reply\": reply,\n",
        "        \"Latency (s)\": latency,\n",
        "        \"Accuracy\": semantic_acc,\n",
        "        \"Visualization Quality\": \"N/A\",\n",
        "        \"Speech Accuracy\": speech_accuracy,\n",
        "        \"Notes\": \"Speech→STT→TinyLlama→TTS pipeline\"\n",
        "    }\n",
        "    speech_results.append(record)\n",
        "\n",
        "# --- Save full metrics ---\n",
        "speech_path = \"audio_outputs/audio_full_metrics.csv\"\n",
        "pd.DataFrame(speech_results).to_csv(speech_path, index=False)\n",
        "\n",
        "\n",
        "# --- Summary view ---\n",
        "df_speech = pd.DataFrame(speech_results)\n",
        "avg_latency = round(df_speech[\"Latency (s)\"].mean(), 2)\n",
        "avg_sem_acc = round(df_speech[\"Accuracy\"].mean(), 2)\n",
        "avg_speech_acc = round(df_speech[\"Speech Accuracy\"].mean(), 2)\n",
        "print(f\"\\nAverage Speech Latency: {avg_latency}s\")\n",
        "print(f\"Average Semantic Accuracy: {avg_sem_acc}\")\n",
        "print(f\"Average Speech Accuracy (STT): {avg_speech_acc}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "42sKl8TS-pzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization Evaluation Cell (Track B)"
      ],
      "metadata": {
        "id": "0pY6dYnK_O-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "os.makedirs(\"visual_outputs\", exist_ok=True)\n",
        "\n",
        "# --- Step 1: Load dataset (must have game_number, accuracy, loss, moves_analyzed)\n",
        "df = pd.read_csv(\"visual_inputs.csv\")\n",
        "print('\\nView Dataset')\n",
        "display(df.head(3))\n",
        "print(f'\\n\\n')\n",
        "\n",
        "\n",
        "# --- Step 2: NL → Plot Specification ---\n",
        "def nl_to_plot_spec(query):\n",
        "    q = query.lower()\n",
        "    if \"accuracy\" in q and \"loss\" in q:\n",
        "        return {\"x\": \"game_number\", \"y\": [\"accuracy\", \"loss\"]}\n",
        "    elif \"accuracy\" in q:\n",
        "        return {\"x\": \"game_number\", \"y\": \"accuracy\"}\n",
        "    elif \"loss\" in q:\n",
        "        return {\"x\": \"game_number\", \"y\": \"loss\"}\n",
        "    elif \"moves\" in q:\n",
        "        return {\"x\": \"game_number\", \"y\": \"moves_analyzed\"}\n",
        "    elif \"moves\" in q:\n",
        "        return {\"x\": \"loss\", \"y\":\"moves_analyzed\"}\n",
        "    else:\n",
        "        return {\"x\": \"game_number\", \"y\": \"accuracy\"}\n",
        "\n",
        "\n",
        "# --- Step 3: Plotting function ---\n",
        "def plot_from_spec(spec, title_suffix=\"\"):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    if isinstance(spec[\"y\"], list):\n",
        "        for y_key in spec[\"y\"]:\n",
        "            plt.plot(df[spec[\"x\"]], df[y_key], label=y_key.capitalize())\n",
        "        plt.legend()\n",
        "    else:\n",
        "        plt.plot(df[spec[\"x\"]], df[spec[\"y\"]])\n",
        "    plt.xlabel(spec[\"x\"].replace(\"_\", \" \").title())\n",
        "    plt.ylabel(\n",
        "        \", \".join(spec[\"y\"]) if isinstance(spec[\"y\"], list) else spec[\"y\"].title()\n",
        "    )\n",
        "    title = f\"{', '.join(spec['y']) if isinstance(spec['y'], list) else spec['y'].title()} vs {spec['x'].title()} {title_suffix}\"\n",
        "    plt.title(title)\n",
        "    path = f\"visual_outputs/{title.replace(' ', '_').lower()}.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "# --- Step 4: Visualization accuracy (keyword overlap) ---\n",
        "def viz_accuracy(query, spec):\n",
        "    q = query.lower()\n",
        "    if isinstance(spec[\"y\"], list):\n",
        "        hits = sum(1 for v in spec[\"y\"] if v in q)\n",
        "        return round(hits / len(spec[\"y\"]), 2)\n",
        "    hits = 1 if spec[\"y\"] in q else 0\n",
        "    return round(hits, 2)\n",
        "\n",
        "# --- Step 5: Visualization quality metric ---\n",
        "def viz_quality(acc):\n",
        "    if acc >= 0.9:\n",
        "        return \"Very Good\"\n",
        "    elif acc >= 0.75:\n",
        "        return \"Good\"\n",
        "    elif acc >= 0.5:\n",
        "        return \"Fair\"\n",
        "    else:\n",
        "        return \"Poor\"\n",
        "\n",
        "# --- Step 6: Natural-language queries ---\n",
        "viz_queries = [\n",
        "    \"Plot move-prediction accuracy by game number\",\n",
        "    \"Show how loss throughout the game\",\n",
        "    \"Graph the accuracy and loss throughout the game\"\n",
        "]\n",
        "\n",
        "# --- Step 7: Run all queries and store full metrics ---\n",
        "results = []\n",
        "for q in viz_queries:\n",
        "    start = time.time()\n",
        "    spec = nl_to_plot_spec(q)\n",
        "    path = plot_from_spec(spec)\n",
        "    latency = round(time.time() - start, 2)\n",
        "    acc = viz_accuracy(q, spec)\n",
        "    quality = viz_quality(acc)\n",
        "\n",
        "    record = {\n",
        "        \"Variant\": \"With_Visualization\",\n",
        "        \"Question\": q,\n",
        "        \"Gold Answer\": \"N/A\",\n",
        "        \"Reply\": \"N/A\",\n",
        "        \"Latency (s)\": latency,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Visualization Quality\": quality,\n",
        "        \"Speech Accuracy\": \"N/A\",\n",
        "        \"Notes\": \"NL→Plot Spec → Matplotlib Chart\",\n",
        "    }\n",
        "    results.append(record)\n",
        "    print(f\"\\nQuery: {q}\\n  * Spec: {spec}\\n  * Chart saved: {path}\")\n",
        "\n",
        "\n",
        "# --- Step 8: Save summary CSV ---\n",
        "df_visual = pd.DataFrame(results)\n",
        "viz_path = \"visual_outputs/visualization_full_metrics.csv\"\n",
        "df_visual.to_csv(viz_path, index=False)\n",
        "\n",
        "\n",
        "# --- Step 9: Summary view ---\n",
        "avg_latency = round(df_visual[\"Latency (s)\"].mean(), 2)\n",
        "avg_acc = round(df_visual[\"Accuracy\"].mean(), 2)\n",
        "print(f\"\\n\\n\\nAverage Visualization Latency: {avg_latency}s\")\n",
        "print(f\"Average Visualization Accuracy: {avg_acc}\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xArbLesU8ixl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Ablation Table\n",
        "\n"
      ],
      "metadata": {
        "id": "OpKJOHbN8vQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_full = pd.concat([df_text, df_speech, df_visual], ignore_index=True)\n",
        "\n",
        "ablation=pd.DataFrame(df_full)\n",
        "ablation.to_csv(\"week8_ablation_results.csv\",index=False)\n",
        "ablation\n"
      ],
      "metadata": {
        "id": "n33WD9nU80XZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
